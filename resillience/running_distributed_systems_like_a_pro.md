Source: https://www.youtube.com/watch?v=J6mzWQQhppM&ab_channel=TheGeekNarrator

**Practices to follow while designing and developing distributed systems**
1. Solid scalable, resilient design.
2. While designing the architecture, we need to take care of all the layers from high level to low-level algorithms, performance, I/O.

**What do we mean by Scalable design?**
* The software should be able to scale vertically as well as horizontally.
* Example: you may double the nodes, but that may not necessarily double the performance because of the design.
* Data intenstive systems tend to be memory and I/O intensive rather than CPU.

**Safety nets for developing distributed systems**
1. Unit tests, integrations tests.
2. Performance benchmarking for critical flows.

**When is the right time to do performance benchmarking**
* It has a certain cost to it. We cannot do it everyday.
* We need to have awareness of the critical paths for the reads and writes. Those are the paths which can consume quite a lot of resources and we need to confirm the SLAs and the contracts.
* *People processing technology*: Every commit or group of commits get certified for performance. Release certification process.
* Embracing failures and having safety nets.

**Release certification**
* It should be as much automated as possible. Eg. Test suites check if there is a non-optional field added which makes the code backward-incompatible.

**Deployments**
* Canary might not be a good idea for stateful systems, where the traffic only goes to a subset of users.
    * Reason: In stateless applications, all the nodes are same i.e. a request can be handled by any node. While in case of stateful system, the required data might or might not be present in a node.
* Having trackability is very critical for oncall engineers to understand that who did the change and when was it done.
* Having features flags or feature toggles for rollbacks is crucial.

**How do we observe our system post deployment?**
* Observability, operability, supportability, debuggability
* Pushing the right set of metrics from the code that will improve the observability of the system.
* The person building the software and the person operating it might not be the same.
* *Application level metrics*: Example for Pinot
    * We should be able to triage an issue just by looking at a metric.
    * Query latency metric depending on the SLA.
        * Read QPS spike.
        * No. of documents scanned during filter, aggregation phase, etc.
        * The selectivity of the queries, the no. of groups generated by the query.
    * We can see which one of the above metric is also showing a spike around that time.

**How do we avoid alerting (on-call) noise**
* To start with, we should have alerts only on the SLAs that we are breaking.
* Eg. API latency could be impacting multiple other alerts too. Those should be found directly on the dashboard for triaging rather than having alerts for each.
* SLOs being slightly tighter than the SLAs so that we come to know when something bad is going to happen.